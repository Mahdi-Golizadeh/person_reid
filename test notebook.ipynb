{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecde7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "sys.path.append('.')\n",
    "from Config import *\n",
    "from datas import make_data_loader\n",
    "from modeling import build_model\n",
    "from solver import make_optimizer, make_optimizer_with_center, WarmupMultiStepLR\n",
    "from losses import make_loss, make_loss_with_center\n",
    "from trainer import do_train, do_train_with_center\n",
    "from logger import setup_logger\n",
    "MODEL_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f31c9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:07:24,476 reid_baseline INFO: Using 1 GPUS\n",
      "=> DukeMTMC-reID loaded\n",
      "Dataset statistics:\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    57 |     1320 |         8\n",
      "  query    |    89 |      300 |         8\n",
      "  gallery  |    61 |     1269 |         8\n",
      "  ----------------------------------------\n",
      "Loading pretrained ImageNet model......\n",
      "Train without center loss, the loss type is triplet\n",
      "2024-03-12 15:07:27,396 reid_baseline.train INFO: Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahdi\\anaconda3\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:996: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.4.14.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\mahdi\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 140\u001b[0m\n\u001b[0;32m    136\u001b[0m     train()\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 140\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 136\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MODEL_DEVICE_ID    \u001b[38;5;66;03m# new add by gu\u001b[39;00m\n\u001b[0;32m    135\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly support pretrain_choice for imagenet and self, but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     34\u001b[0m             MODEL_PRETRAIN_CHOICE))\n\u001b[0;32m     36\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 38\u001b[0m     do_train(\n\u001b[0;32m     39\u001b[0m         model,\n\u001b[0;32m     40\u001b[0m         train_loader,\n\u001b[0;32m     41\u001b[0m         val_loader,\n\u001b[0;32m     42\u001b[0m         optimizer,\n\u001b[0;32m     43\u001b[0m         scheduler,      \u001b[38;5;66;03m# modify for using self trained model\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         loss_func,\n\u001b[0;32m     45\u001b[0m         num_query,\n\u001b[0;32m     46\u001b[0m         start_epoch     \u001b[38;5;66;03m# add for using self trained model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m MODEL_IF_WITH_CENTER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain with center loss, the loss type is\u001b[39m\u001b[38;5;124m'\u001b[39m, MODEL_METRIC_LOSS_TYPE)\n",
      "File \u001b[1;32mE:\\person reid\\reid-strong-baseline-master\\my_custom_model\\trainer.py:413\u001b[0m, in \u001b[0;36mdo_train\u001b[1;34m(model, train_loader, val_loader, optimizer, scheduler, loss_fn, num_query, start_epoch)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]:\n\u001b[0;32m    411\u001b[0m             logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCMC curve, Rank-\u001b[39m\u001b[38;5;132;01m{:<3}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{:.1%}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(r, cmc[r \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m--> 413\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun(train_loader, max_epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:898\u001b[0m, in \u001b[0;36mEngine.run\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[1;32m--> 898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run()\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:941\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:999\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(e)\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:965\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[1;32m--> 965\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:1074\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[1;32m-> 1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "File \u001b[1;32mE:\\person reid\\reid-strong-baseline-master\\my_custom_model\\trainer.py:254\u001b[0m, in \u001b[0;36mcreate_supervised_trainer.<locals>._update\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m    252\u001b[0m score, feat \u001b[38;5;241m=\u001b[39m model(img)\n\u001b[0;32m    253\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(score, feat, target)\n\u001b[1;32m--> 254\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    255\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# compute acc\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # prepare dataset\n",
    "    train_loader, val_loader, num_query, num_classes = make_data_loader()\n",
    "\n",
    "    # prepare model\n",
    "    model = build_model(num_classes)\n",
    "\n",
    "    if MODEL_IF_WITH_CENTER == 'no':\n",
    "        print('Train without center loss, the loss type is', MODEL_METRIC_LOSS_TYPE)\n",
    "        optimizer = make_optimizer(model)\n",
    "        # scheduler = WarmupMultiStepLR(optimizer, cfg.SOLVER.STEPS, cfg.SOLVER.GAMMA, cfg.SOLVER.WARMUP_FACTOR,\n",
    "        #                               cfg.SOLVER.WARMUP_ITERS, cfg.SOLVER.WARMUP_METHOD)\n",
    "\n",
    "        loss_func = make_loss(num_classes)     # modified by gu\n",
    "\n",
    "        # Add for using self trained model\n",
    "        if MODEL_PRETRAIN_CHOICE == 'self':\n",
    "            start_epoch = eval(MODEL_PRETRAIN_PATH.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            print('Start epoch:', start_epoch)\n",
    "            path_to_optimizer = MODEL_PRETRAIN_PATH.replace('model', 'optimizer')\n",
    "            print('Path to the checkpoint of optimizer:', path_to_optimizer)\n",
    "            model.load_state_dict(torch.load(MODEL_PRETRAIN_PATH))\n",
    "            optimizer.load_state_dict(torch.load(path_to_optimizer))\n",
    "            scheduler = WarmupMultiStepLR(optimizer, SOLVER_STEPS, \n",
    "                                          SOLVER_GAMMA, SOLVER_WARMUP_FACTOR,\n",
    "                                          SOLVER_WARMUP_ITERS, SOLVER_WARMUP_METHOD, start_epoch)\n",
    "        elif MODEL_PRETRAIN_CHOICE == 'imagenet':\n",
    "            start_epoch = 0\n",
    "            scheduler = WarmupMultiStepLR(optimizer, SOLVER_STEPS, \n",
    "                                          SOLVER_GAMMA, SOLVER_WARMUP_FACTOR,\n",
    "                                          SOLVER_WARMUP_ITERS, SOLVER_WARMUP_METHOD)\n",
    "        else:\n",
    "            print('Only support pretrain_choice for imagenet and self, but got {}'.format(\n",
    "                MODEL_PRETRAIN_CHOICE))\n",
    "\n",
    "        arguments = {}\n",
    "\n",
    "        do_train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            scheduler,      # modify for using self trained model\n",
    "            loss_func,\n",
    "            num_query,\n",
    "            start_epoch     # add for using self trained model\n",
    "        )\n",
    "    elif MODEL_IF_WITH_CENTER == 'yes':\n",
    "        print('Train with center loss, the loss type is', MODEL_METRIC_LOSS_TYPE)\n",
    "        loss_func, center_criterion = make_loss_with_center(num_classes)  # modified by gu\n",
    "        optimizer, optimizer_center = make_optimizer_with_center(model, center_criterion)\n",
    "        # scheduler = WarmupMultiStepLR(optimizer, SOLVER_STEPS, \n",
    "#                                         SOLVER_GAMMA, SOLVER_WARMUP_FACTOR,\n",
    "        #                               SOLVER_WARMUP_ITERS, SOLVER_WARMUP_METHOD)\n",
    "\n",
    "        arguments = {}\n",
    "\n",
    "        # Add for using self trained model\n",
    "        if MODEL_PRETRAIN_CHOICE == 'self':\n",
    "            start_epoch = eval(MODEL_PRETRAIN_PATH.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            print('Start epoch:', start_epoch)\n",
    "            path_to_optimizer = MODEL_PRETRAIN_PATH.replace('model', 'optimizer')\n",
    "            print('Path to the checkpoint of optimizer:', path_to_optimizer)\n",
    "            path_to_center_param = MODEL_PRETRAIN_PATH.replace('model', 'center_param')\n",
    "            print('Path to the checkpoint of center_param:', path_to_center_param)\n",
    "            path_to_optimizer_center = MODEL_PRETRAIN_PATH.replace('model', 'optimizer_center')\n",
    "            print('Path to the checkpoint of optimizer_center:', path_to_optimizer_center)\n",
    "            model.load_state_dict(torch.load(MODEL_PRETRAIN_PATH))\n",
    "            optimizer.load_state_dict(torch.load(path_to_optimizer))\n",
    "            center_criterion.load_state_dict(torch.load(path_to_center_param))\n",
    "            optimizer_center.load_state_dict(torch.load(path_to_optimizer_center))\n",
    "            scheduler = WarmupMultiStepLR(optimizer, SOLVER_STEPS, \n",
    "                                          SOLVER_GAMMA, SOLVER_WARMUP_FACTOR,\n",
    "                                          SOLVER_WARMUP_ITERS, SOLVER_WARMUP_METHOD, start_epoch)\n",
    "        elif MODEL_PRETRAIN_CHOICE == 'imagenet':\n",
    "            start_epoch = 0\n",
    "            scheduler = WarmupMultiStepLR(optimizer, SOLVER_STEPS, \n",
    "                                          SOLVER_GAMMA, SOLVER_WARMUP_FACTOR,\n",
    "                                          SOLVER_WARMUP_ITERS, SOLVER_WARMUP_METHOD)\n",
    "        else:\n",
    "            print('Only support pretrain_choice for imagenet and self, but got {}'.format(\n",
    "                MODEL_PRETRAIN_CHOICE))\n",
    "\n",
    "        do_train_with_center(\n",
    "            model,\n",
    "            center_criterion,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            optimizer_center,\n",
    "            scheduler,      # modify for using self trained model\n",
    "            loss_func,\n",
    "            num_query,\n",
    "            start_epoch     # add for using self trained model\n",
    "        )\n",
    "    else:\n",
    "        print(\"Unsupported value for cfg.MODEL.IF_WITH_CENTER {}, only support yes or no!\\n\".format(\n",
    "            MODEL_IF_WITH_CENTER))\n",
    "\n",
    "\n",
    "def main():\n",
    "#     parser = argparse.ArgumentParser(description=\"ReID Baseline Training\")\n",
    "#     parser.add_argument(\n",
    "#         \"--config_file\", default=\"\", help=\"path to config file\", type=str\n",
    "#     )\n",
    "#     parser.add_argument(\"opts\", help=\"Modify config options using the command-line\", default=None,\n",
    "#                         nargs=argparse.REMAINDER)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
    "\n",
    "#     if args.config_file != \"\":\n",
    "#         merge_from_file(args.config_file)\n",
    "#     merge_from_list(args.opts)\n",
    "#     freeze()\n",
    "\n",
    "    output_dir = OUTPUT_DIR\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    logger = setup_logger(\"reid_baseline\", output_dir, 0)\n",
    "    logger.info(\"Using {} GPUS\".format(num_gpus))\n",
    "#     logger.info(args)\n",
    "\n",
    "#     if args.config_file != \"\":\n",
    "#         logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
    "#         with open(args.config_file, 'r') as cf:\n",
    "#             config_str = \"\\n\" + cf.read()\n",
    "#             logger.info(config_str)\n",
    "#     logger.info(\"Running with config:\\n{}\".format())\n",
    "\n",
    "    if MODEL_DEVICE == \"cuda\":\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = MODEL_DEVICE_ID    # new add by gu\n",
    "    cudnn.benchmark = True\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584464a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
